{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import RobertaModel, RobertaTokenizer\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import collections\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using roberta\n",
      "Reading: /data2/yikyungkim/dataset/finqa_original/operation_list.txt\n",
      "Reading: /data2/yikyungkim/dataset/finqa_original/constant_list.txt\n"
     ]
    }
   ],
   "source": [
    "from config import parameters as conf\n",
    "import sampling as sampling\n",
    "from Main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaConfig\n",
    "tokenizer = RobertaTokenizer.from_pretrained(conf.model_size)\n",
    "model_config = RobertaConfig.from_pretrained(conf.model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir_name = conf.model_save_name\n",
    "model_dir = os.path.join(conf.output_path, model_dir_name)\n",
    "results_path = os.path.join(model_dir, \"results\")\n",
    "saved_model_path = os.path.join(model_dir, \"saved_model\")\n",
    "os.makedirs(saved_model_path, exist_ok=True)       # for restart \n",
    "os.makedirs(results_path, exist_ok=True)\n",
    "log_file = os.path.join(results_path, 'log.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: /data2/yikyungkim/dataset/finqa_original/operation_list.txt\n",
      "Reading: /data2/yikyungkim/dataset/finqa_original/constant_list.txt\n"
     ]
    }
   ],
   "source": [
    "op_list = read_txt(conf.op_list_file, log_file)\n",
    "op_list = [op + '(' for op in op_list]\n",
    "op_list = ['EOF', 'UNK', 'GO', ')'] + op_list\n",
    "const_list = read_txt(conf.const_list_file, log_file)\n",
    "const_list = [const.lower().replace('.', '_') for const in const_list]\n",
    "reserved_token_size = len(op_list) + len(const_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token = {'additional_special_tokens': ['[QNP]']}\n",
    "num_added_toks = tokenizer.add_special_tokens(special_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /data2/yikyungkim/dataset/finqa_retriever_output/train_retrieve.json\n",
      "starts loading question score\n",
      "starts loading program score and gold indices\n",
      "starts loading question similar candidates\n"
     ]
    }
   ],
   "source": [
    "train_data = load_data(conf.train_file, log_file)\n",
    "kwargs_load={\n",
    "    'finqa_dataset_path': conf.train_case,\n",
    "    'constants_path': conf.const_list_file,\n",
    "    'archive_path': conf.archive_path,\n",
    "    'mode': 'train',\n",
    "    'q_score_available': conf.q_score_available,\n",
    "    'p_score_available': conf.p_score_available,\n",
    "    'candidates_available': conf.candidates_available,\n",
    "    'pos_pool': conf.pos_pool,\n",
    "    'neg_pool': conf.neg_pool\n",
    "}\n",
    "input_data, q_scores, p_scores, gold_indices, constants, gold_cands, non_gold_cands = sampling.load_dataset(**kwargs_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs_sample={\n",
    "    'seed': 0,\n",
    "    'input_data': input_data,\n",
    "    'p_scores': p_scores,\n",
    "    'gold_cands': gold_cands,\n",
    "    'non_gold_cands': non_gold_cands,\n",
    "    'num_case': conf.num_case,\n",
    "    'top3_precision': conf.top3_precision_val\n",
    "}\n",
    "train_case = sampling.get_random_cases(**kwargs_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_examples, op_list, const_list = \\\n",
    "    read_examples(input_data=train_data, case_data=train_case, tokenizer=tokenizer,\n",
    "                op_list=op_list, const_list=const_list, log_file=log_file, \n",
    "                num_case=conf.num_case, input_concat=conf.input_concat, program_type=conf.program_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n"
     ]
    }
   ],
   "source": [
    "kwargs = {\"examples\": train_examples,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"max_seq_length\": conf.max_seq_length,\n",
    "        \"max_program_length\": conf.max_program_length,\n",
    "        \"is_training\": True,\n",
    "        \"op_list\": op_list,\n",
    "        \"op_list_size\": len(op_list),\n",
    "        \"const_list\": const_list,\n",
    "        \"const_list_size\": len(const_list),\n",
    "        \"verbose\": True}\n",
    "\n",
    "train_features = convert_examples_to_features(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /data2/yikyungkim/dataset/finqa_retriever_output/dev_retrieve.json\n",
      "Reading /data2/yikyungkim/case_retriever/inference/cross_roberta-base_q+p_100200/generator_input/valid_300/results/predictions.json\n"
     ]
    }
   ],
   "source": [
    "valid_data = load_data(conf.valid_file, log_file)\n",
    "valid_case = load_data(conf.valid_case, log_file)\n",
    "\n",
    "valid_examples, op_list, const_list = \\\n",
    "    read_examples(input_data=valid_data, case_data=valid_case, tokenizer=tokenizer,\n",
    "                op_list=op_list, const_list=const_list, log_file=log_file, \n",
    "                num_case=conf.num_case, input_concat=conf.input_concat, program_type=conf.program_type)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n",
      "too long\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Convert to model input features \"\"\"\n",
    "kwargs = {\"examples\": train_examples,\n",
    "        \"tokenizer\": tokenizer,\n",
    "        \"max_seq_length\": conf.max_seq_length,\n",
    "        \"max_program_length\": conf.max_program_length,\n",
    "        \"is_training\": True,\n",
    "        \"op_list\": op_list,\n",
    "        \"op_list_size\": len(op_list),\n",
    "        \"const_list\": const_list,\n",
    "        \"const_list_size\": len(const_list),\n",
    "        \"verbose\": True}\n",
    "\n",
    "train_features = convert_examples_to_features(**kwargs)\n",
    "kwargs[\"examples\"] = valid_examples\n",
    "kwargs[\"is_training\"] = False\n",
    "valid_features = convert_examples_to_features(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'divide(19557, const_1000), divide(2.2, #0), EOF'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_case[0]['case_retrieved'][0]['program']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'divide(100, 100), divide(3.8, #0)'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_case[0]['qa']['program']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def program_tokenization(original_program):\n",
    "    original_program = original_program.split(', ')\n",
    "    program = []\n",
    "    for tok in original_program:\n",
    "        cur_tok = ''\n",
    "        for c in tok:\n",
    "            if c == ')':\n",
    "                if cur_tok != '':\n",
    "                    program.append(cur_tok)\n",
    "                    cur_tok = ''\n",
    "            cur_tok += c\n",
    "            if c in ['(', ')']:\n",
    "                program.append(cur_tok)\n",
    "                cur_tok = ''\n",
    "        if cur_tok != '':\n",
    "            program.append(cur_tok)\n",
    "    program.append('EOF')\n",
    "    return program\n",
    "\n",
    "# compute levenshtein distance\n",
    "def levenshtein(s1, s2, debug=False):\n",
    "    if len(s1) < len(s2):\n",
    "        return levenshtein(s2, s1, debug)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    \n",
    "    previous_row = range(len(s2) + 1)\n",
    "    for i, c1 in enumerate(s1):\n",
    "        current_row = [i + 1]\n",
    "        for j, c2 in enumerate(s2):\n",
    "            insertions = previous_row[j + 1] + 1\n",
    "            deletions = current_row[j] + 1\n",
    "            substitutions = previous_row[j] + (c1 != c2)\n",
    "            current_row.append(min(insertions, deletions, substitutions))\n",
    "        if debug:\n",
    "            print(current_row[1:])\n",
    "        previous_row = current_row\n",
    "\n",
    "    return previous_row[-1]\n",
    "\n",
    "# get operators in program\n",
    "def operator(program):\n",
    "    ops=[]\n",
    "    i=0\n",
    "    while i < len(program):\n",
    "        if program[i]!='EOF':\n",
    "            ops.append(program[i])\n",
    "        i+=4\n",
    "    return ops\n",
    "\n",
    "# get arguments in program\n",
    "def arguments(program, constants):\n",
    "    args=[]\n",
    "    i=0\n",
    "    while i < len(program):\n",
    "        if i%4==1:\n",
    "            if program[i] not in constants:\n",
    "                args.append('arg1')\n",
    "            else:\n",
    "                # args.append(program[i])\n",
    "                if 'const' in program[i]:\n",
    "                    args.append('const')\n",
    "                else:\n",
    "                    args.append(program[i])\n",
    "        elif i%4==2:\n",
    "            if program[i] not in constants:\n",
    "                args.append('arg2')\n",
    "            else:\n",
    "                # args.append(program[i])\n",
    "                if 'const' in program[i]:\n",
    "                    args.append('const')\n",
    "                else:\n",
    "                    args.append(program[i])\n",
    "        i+=1\n",
    "    return args\n",
    "\n",
    "# compute program score\n",
    "def distance_score(query, cand, constants, weight):\n",
    "    query_program = program_tokenization(query)\n",
    "    cand_program = program_tokenization(cand)\n",
    "\n",
    "    ops_q = operator(query_program)\n",
    "    ops_c = operator(cand_program)\n",
    "    ops_len = len(ops_q)\n",
    "    ops_distance = levenshtein(ops_q, ops_c)\n",
    "    ops_score = (ops_len - ops_distance)/ops_len\n",
    "\n",
    "    arg_q = arguments(query_program, constants)\n",
    "    arg_c = arguments(cand_program, constants)\n",
    "    arg_len = len(arg_q)\n",
    "    arg_distance = levenshtein(arg_q, arg_c)\n",
    "    arg_score = (arg_len - arg_distance)/arg_len\n",
    "    \n",
    "    return weight * ops_score + (1-weight) * arg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
