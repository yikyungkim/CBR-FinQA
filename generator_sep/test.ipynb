{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/s3/yikyungkim/research/cbr/generator_sep\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/s3/yikyungkim/research/cbr/generator_sep'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd '/home/s3/yikyungkim/research/cbr/generator_sep'\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import logging\n",
    "from utils import *\n",
    "from config import parameters as conf\n",
    "from torch import nn\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "from transformers import RobertaConfig\n",
    "tokenizer = RobertaTokenizer.from_pretrained(conf.model_size)\n",
    "model_config = RobertaConfig.from_pretrained(conf.model_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if conf.mode == \"train\":\n",
    "    # model_dir_name = conf.model_save_name + \"_\" + \\\n",
    "    #     datetime.now().strftime(\"%Y%m%d%H%M%S\")         # for restart\n",
    "    model_dir_name = conf.model_save_name\n",
    "    model_dir = os.path.join(conf.output_path, model_dir_name)\n",
    "    results_path = os.path.join(model_dir, \"results\")\n",
    "    saved_model_path = os.path.join(model_dir, \"saved_model\")\n",
    "    os.makedirs(saved_model_path, exist_ok=True)       # for restart \n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "    log_file = os.path.join(results_path, 'log.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading: /shared/s3/lab07/yikyung/cbr/dataset/finqa_original/operation_list.txt\n",
      "Reading: /shared/s3/lab07/yikyung/cbr/dataset/finqa_original/constant_list.txt\n"
     ]
    }
   ],
   "source": [
    "op_list = read_txt(conf.op_list_file, log_file)\n",
    "op_list = [op + '(' for op in op_list]\n",
    "op_list = ['EOF', 'UNK', 'GO', ')'] + op_list\n",
    "const_list = read_txt(conf.const_list_file, log_file)\n",
    "const_list = [const.lower().replace('.', '_') for const in const_list]\n",
    "reserved_token_size = len(op_list) + len(const_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/ubuntu/yikyung/dataset/finqa_retriever_output/train_retrieve.json\n",
      "Reading /home/ubuntu/yikyung/dataset/case_retriever_output/bi_encoder/train_retrieved_noise.json\n"
     ]
    }
   ],
   "source": [
    "train_data, train_examples, op_list, const_list = \\\n",
    "    read_examples(input_path=conf.train_file, case_path=conf.train_case, tokenizer=tokenizer,\n",
    "                  op_list=op_list, const_list=const_list, log_file=log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /shared/s3/lab07/yikyung/cbr/dataset/finqa_retriever_output/dev_retrieve.json\n",
      "Reading /shared/s3/lab07/yikyung/cbr/dataset/case_retriever_output/cross_encoder/dev_retrieved_noise1_L.json\n"
     ]
    }
   ],
   "source": [
    "valid_data, valid_examples, op_list, const_list = \\\n",
    "    read_examples(input_path=conf.valid_file, case_path=conf.valid_case, tokenizer=tokenizer,\n",
    "                  op_list=op_list, const_list=const_list, log_file=log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"examples\": valid_examples,\n",
    "          \"tokenizer\": tokenizer,\n",
    "          \"max_seq_length\": conf.max_seq_length,\n",
    "          \"max_program_length\": conf.max_program_length,\n",
    "          \"is_training\": True,\n",
    "          \"op_list\": op_list,\n",
    "          \"op_list_size\": len(op_list),\n",
    "          \"const_list\": const_list,\n",
    "          \"const_list_size\": len(const_list),\n",
    "          \"verbose\": True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_features = convert_examples_to_features(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = valid_features[0].input_ids\n",
    "input_mask = valid_features[0].input_mask\n",
    "segment_ids = valid_features[0].segment_ids\n",
    "option_mask = valid_features[0].option_mask\n",
    "program_ids = valid_features[0].program_ids\n",
    "program_mask = valid_features[0].program_mask\n",
    "options = valid_features[0].options\n",
    "answer = valid_features[0].answer\n",
    "program = valid_features[0].program\n",
    "program_weight = valid_features[0].program_weight\n",
    "case_ids = valid_features[0].case_ids\n",
    "case_mask = valid_features[0].case_mask\n",
    "case_segs = valid_features[0].case_segs\n",
    "\n",
    "\n",
    "input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
    "input_mask = torch.tensor(input_mask).unsqueeze(0)\n",
    "segment_ids = torch.tensor(segment_ids).unsqueeze(0)\n",
    "program_ids = torch.tensor(program_ids).unsqueeze(0)\n",
    "program_mask = torch.tensor(program_mask).unsqueeze(0)\n",
    "option_mask = torch.tensor(option_mask).unsqueeze(0)\n",
    "case_ids = torch.tensor(case_ids).unsqueeze(0)\n",
    "case_mask = torch.tensor(case_mask).unsqueeze(0)\n",
    "case_segs = torch.tensor(case_segs).unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1068])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "option_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model_new import Bert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3fd20795604e91a75f96214efb8bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)lve/main/config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65141ee0235944aeb77a68c60da2484b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/1.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::mkldnn_rnn_layer' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::mkldnn_rnn_layer' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31034 [kernel]\nMeta: registered at /dev/null:241 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:280 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:16726 [kernel]\nAutocastCPU: registered at ../aten/src/ATen/autocast_mode.cpp:492 [kernel]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:815 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1073 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/s3/yikyungkim/research/cbr/generator_sep/test.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B147.47.200.22/home/s3/yikyungkim/research/cbr/generator_sep/test.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B147.47.200.22/home/s3/yikyungkim/research/cbr/generator_sep/test.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B147.47.200.22/home/s3/yikyungkim/research/cbr/generator_sep/test.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     logits \u001b[39m=\u001b[39m model(\u001b[39mFalse\u001b[39;00m, input_ids, input_mask, segment_ids, case_ids, case_mask, case_segs,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B147.47.200.22/home/s3/yikyungkim/research/cbr/generator_sep/test.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m                     option_mask, program_ids, program_mask, device\u001b[39m=\u001b[39mconf\u001b[39m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/research/cbr/generator_sep/Model_new.py:339\u001b[0m, in \u001b[0;36mBert_model.forward\u001b[0;34m(self, is_training, input_ids, input_mask, segment_ids, case_ids, case_mask, case_segs, option_mask, program_ids, program_mask, device)\u001b[0m\n\u001b[1;32m    333\u001b[0m     program_index \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrepeat_interleave(\n\u001b[1;32m    334\u001b[0m         program_index, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)  \u001b[39m# [batch, 1, hidden]\u001b[39;00m\n\u001b[1;32m    336\u001b[0m     input_program_embeddings \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mgather(\n\u001b[1;32m    337\u001b[0m         option_embeddings, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, index\u001b[39m=\u001b[39mprogram_index)\n\u001b[0;32m--> 339\u001b[0m     decoder_output, (decoder_state_h, decoder_state_c) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn(\n\u001b[1;32m    340\u001b[0m         input_program_embeddings, (decoder_state_h, decoder_state_c))\n\u001b[1;32m    341\u001b[0m     decoder_history \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(\n\u001b[1;32m    342\u001b[0m         [decoder_history, input_program_embeddings], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    344\u001b[0m logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(logits, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n\u001b[1;32m    814\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    816\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::mkldnn_rnn_layer' with arguments from the 'CUDA' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::mkldnn_rnn_layer' is only available for these backends: [CPU, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMeta, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31034 [kernel]\nMeta: registered at /dev/null:241 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:144 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:491 [backend fallback]\nFunctionalize: registered at ../aten/src/ATen/FunctionalizeFallbackKernel.cpp:280 [backend fallback]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ../aten/src/ATen/core/VariableFallbackKernel.cpp:63 [backend fallback]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_2.cpp:17472 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_2.cpp:16726 [kernel]\nAutocastCPU: registered at ../aten/src/ATen/autocast_mode.cpp:492 [kernel]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:354 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:815 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1073 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:210 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:152 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:487 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:148 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "model = Bert_model(num_decoder_layers=conf.num_decoder_layers,\n",
    "                    hidden_size=model_config.hidden_size,\n",
    "                    dropout_rate=conf.dropout_rate,\n",
    "                    program_length=conf.max_program_length,\n",
    "                    input_length=conf.max_seq_length,\n",
    "                    op_list=op_list,\n",
    "                    const_list=const_list,\n",
    "                    tokenizer=tokenizer)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(False, input_ids, input_mask, segment_ids, case_ids, case_mask, case_segs,\n",
    "                    option_mask, program_ids, program_mask, device=conf.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 768])\n",
      "torch.Size([1, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "bert_outputs = model.bert(input_ids=input_ids, attention_mask=input_mask, token_type_ids=segment_ids)\n",
    "bert_sequence_output = bert_outputs.last_hidden_state\n",
    "bert_pooled_output = bert_sequence_output[:, 0, :]\n",
    "batch_size, seq_length, bert_dim = list(bert_sequence_output.size())\n",
    "\n",
    "print(bert_sequence_output.size())\n",
    "\n",
    "pooled_output = model.cls_prj(bert_pooled_output)\n",
    "pooled_output = model.cls_dropout(pooled_output)\n",
    "\n",
    "sequence_output = model.seq_prj(bert_sequence_output)\n",
    "sequence_output = model.seq_dropout(sequence_output)\n",
    "print(sequence_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "case_bert_outputs = model.bert(input_ids = case_ids, attention_mask=case_mask, token_type_ids=case_segs)\n",
    "case_bert_sequence_output = case_bert_outputs.last_hidden_state\n",
    "case_sequence_output = model.case_seq_prj(case_bert_sequence_output)\n",
    "case_sequence_output = model.case_seq_dropout(case_bert_sequence_output)\n",
    "\n",
    "print(case_sequence_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "torch.Size([1, 30])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "split_program_ids = torch.split(program_ids, 1, dim=1)\n",
    "print(model.program_length)\n",
    "print(program_ids.size())\n",
    "print(split_program_ids[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([44, 768])\n",
      "torch.Size([1, 44, 768])\n",
      "torch.Size([1, 768])\n",
      "torch.Size([1, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "# option_size = model.reserved_token_size + seq_length\n",
    "\n",
    "op_embeddings = model.reserved_token_embedding(model.reserved_ind)\n",
    "print(op_embeddings.size())\n",
    "op_embeddings = op_embeddings.repeat(batch_size, 1, 1)\n",
    "print(op_embeddings.size())\n",
    "\n",
    "init_decoder_output = model.reserved_token_embedding(model.reserved_go)\n",
    "print(init_decoder_output.size())\n",
    "decoder_output = init_decoder_output.repeat(batch_size, 1, 1)\n",
    "print(decoder_output.size())\n",
    "\n",
    "if conf.sep_attention:\n",
    "    decoder_history = decoder_output\n",
    "else:\n",
    "    decoder_history = torch.unsqueeze(pooled_output, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1068, 768])\n"
     ]
    }
   ],
   "source": [
    "initial_option_embeddings = torch.cat(\n",
    "    [op_embeddings, sequence_output, case_sequence_output], dim=1)\n",
    "print(initial_option_embeddings.size())\n",
    "\n",
    "this_step_new_op_emb = initial_option_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512, 1])\n"
     ]
    }
   ],
   "source": [
    "decoder_state_h = torch.zeros(1, batch_size, model.hidden_size)\n",
    "decoder_state_c = torch.zeros(1, batch_size, model.hidden_size)\n",
    "print(decoder_state_c.size())\n",
    "\n",
    "print(input_mask.size())\n",
    "float_input_mask = input_mask.float()\n",
    "float_input_mask = torch.unsqueeze(float_input_mask, dim=-1)\n",
    "print(float_input_mask.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512])\n",
      "torch.Size([1, 512, 1])\n"
     ]
    }
   ],
   "source": [
    "print(case_mask.size())\n",
    "float_case_mask = case_mask.float()\n",
    "float_case_mask = torch.unsqueeze(float_case_mask, dim=-1)\n",
    "print(float_case_mask.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "decoder_history_attn_vec = model.decoder_history_attn_prj(decoder_output)\n",
    "decoder_history_attn_vec = model.decoder_history_attn_dropout(decoder_history_attn_vec)\n",
    "print(decoder_history_attn_vec.size())\n",
    "\n",
    "decoder_history_attn_w = torch.matmul(decoder_history, torch.transpose(decoder_history_attn_vec, 1, 2))  # (B1H)(BH1)\n",
    "decoder_history_attn_w = F.softmax(decoder_history_attn_w, dim=1)\n",
    "print(decoder_history_attn_w.size()) # B11\n",
    "\n",
    "decoder_history_ctx_embeddings = torch.matmul(torch.transpose(decoder_history_attn_w, 1, 2), decoder_history)\n",
    "print(decoder_history_ctx_embeddings.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "question_attn_vec = model.question_attn_prj(decoder_output)\n",
    "question_attn_vec = model.question_attn_dropout(question_attn_vec)\n",
    "print(question_attn_vec.size())\n",
    "\n",
    "question_attn_w = torch.matmul(sequence_output, torch.transpose(question_attn_vec, 1, 2))\n",
    "question_attn_w -= 1e6 * (1 - float_input_mask)\n",
    "question_attn_w = F.softmax(question_attn_w, dim=1)\n",
    "print(question_attn_w.size())\n",
    "\n",
    "question_ctx_embeddings = torch.matmul(torch.transpose(question_attn_w, 1, 2), sequence_output)\n",
    "print(question_ctx_embeddings.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768])\n",
      "torch.Size([1, 512, 1])\n",
      "torch.Size([1, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "question_summary_vec = model.question_summary_attn_prj(decoder_output)\n",
    "question_summary_vec = model.question_summary_attn_dropout(question_summary_vec)\n",
    "print(question_summary_vec.size())\n",
    "\n",
    "question_summary_w = torch.matmul(sequence_output, torch.transpose(question_summary_vec, 1, 2))\n",
    "question_summary_w -= 1e6 * (1 - float_input_mask)\n",
    "question_summary_w = F.softmax(question_summary_w, dim=1)\n",
    "print(question_summary_w.size())    # [B 512 H]x[B 768 H]\n",
    "\n",
    "question_summary_embeddings = torch.matmul(torch.transpose(question_summary_w, 1, 2), sequence_output)\n",
    "print(question_summary_embeddings.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "case_attn_vec = model.case_attn_prj(decoder_output)\n",
    "case_attn_vec = model.case_attn_dropout(case_attn_vec)\n",
    "\n",
    "case_attn_w = torch.matmul(case_sequence_output, torch.transpose(case_attn_vec, 1, 2)) # attention score\n",
    "case_attn_w -= 1e6 * (1 - float_input_mask)\n",
    "case_attn_w = F.softmax(case_attn_w, dim=1) # attention coefficient\n",
    "\n",
    "case_embeddings = torch.matmul(torch.transpose(case_attn_w, 1, 2), case_sequence_output) # attention value\n",
    "print(case_embeddings.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=3072, out_features=768, bias=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input_embeddings_prj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 3072])\n",
      "torch.Size([1, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "concat_input_embeddings = torch.cat([decoder_history_ctx_embeddings,\n",
    "                                        question_ctx_embeddings,\n",
    "                                        case_embeddings,\n",
    "                                        decoder_output], dim=-1)\n",
    "\n",
    "print(concat_input_embeddings.size())\n",
    "input_embeddings = model.input_embeddings_prj(concat_input_embeddings)\n",
    "input_embeddings = model.input_embeddings_layernorm(input_embeddings)\n",
    "print(input_embeddings.size())\n",
    "\n",
    "# torch.Size([1, 1, 2304])\n",
    "# torch.Size([1, 1, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1068, 768])\n",
      "torch.Size([1, 1068, 1536])\n",
      "torch.Size([1, 1068, 768])\n",
      "torch.Size([1, 1068, 1])\n",
      "torch.Size([1, 1068])\n"
     ]
    }
   ],
   "source": [
    "question_option_vec = this_step_new_op_emb * question_summary_embeddings # H * att_p'\n",
    "print(question_option_vec.size())\n",
    "option_embeddings = torch.cat([this_step_new_op_emb, question_option_vec], dim=-1)  #[H; h*att_p']\n",
    "print(option_embeddings.size())\n",
    "option_embeddings = model.option_embeddings_prj(option_embeddings)\n",
    "print(option_embeddings.size())\n",
    "\n",
    "option_logits = torch.matmul(option_embeddings, torch.transpose(input_embeddings, 1, 2))\n",
    "print(option_logits.size())\n",
    "option_logits = torch.squeeze(option_logits, dim=2)  # [batch, op + seq_len]\n",
    "option_logits -= 1e6 * (1 - option_mask)\n",
    "print(option_logits.size())\n",
    "\n",
    "# torch.Size([1, 556, 1536])\n",
    "# torch.Size([1, 556, 768])\n",
    "# torch.Size([1, 556, 1])\n",
    "# torch.Size([1, 556])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1])\n",
      "torch.Size([1, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "program_index = torch.unsqueeze(\n",
    "    split_program_ids[0], dim=1)\n",
    "print(program_index.size())\n",
    "\n",
    "program_index = torch.repeat_interleave(\n",
    "    program_index, model.hidden_size, dim=2)  # [batch, 1, hidden]\n",
    "print(program_index.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "input_program_embeddings = torch.gather(\n",
    "    option_embeddings, dim=1, index=program_index)\n",
    "print(input_program_embeddings.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 768])\n"
     ]
    }
   ],
   "source": [
    "decoder_output, (decoder_state_h, decoder_state_c) = model.rnn(\n",
    "    input_program_embeddings, (decoder_state_h, decoder_state_c))\n",
    "print(decoder_output.size())\n",
    "\n",
    "decoder_history = torch.cat(\n",
    "    [decoder_history, input_program_embeddings], dim=1)\n",
    "# (1,1,H) + (1,1,H) = (1,2,H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
